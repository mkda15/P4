\section{FFT}
This section describes the implementation of the FFT discussed in section \ref{sec:FFT}, which uses an implementation of the DFT discussed in section \ref{sec:DFT}. The implementation of the FFT is assessed by comparing both the computational efforts and the results of the implementations of the DFT and FFT compared to Numpy's FFT. The results are expected to be similar since the FFT is a fast way of calculating the DFT.

\subsection{Implementation of the FFT}
Firstly, the implementation of the DFT is described in the following algorithm.
\begin{algorithm}
\caption{DFT algorithm}
\label{DFTalg}
\begin{algorithmic}[1]
\State $N = \ $length of $x$ \Comment{Number of samples to be computed}
\Procedure{Compute DFT}{$x,N$}
	\State{$X = \ $np.zeros$(N,$dtype = complex$)$}
	\For{$k$ between 0 and length of signal}
		\State{$a = 0 + 0\cdot j$} \Comment{$a$ is zero but of type complex}
		\For{$n$ between 0 and $N$}
			\State{$a$ += $x[n]\cdot \exp(-2\cdot \pi\cdot j 					\cdot k \cdot n/$float$(c))$}
			\State{$X[k] = a$}
		\EndFor
	\EndFor
	\State {Return $X$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The implementation of the FFT shown in the following algorithm uses a function to make sure that the number of samples to be computed is a power of 2 (as described in section \ref{sec:FFT}). This function asks if $N$ is positive and a power of 2. If not, the function returns a False-statement, which raises a \textit{Valueerror} in the FFT algorithm. However, the FFT function from the Numpy-package uses zeropadding to modify a signal of arbitrary length into a signal with a length that is a power of 2. That has not been implemented in the FFT shown below. In the FFT algorithm shown below, the signal is divided into even and odd samples if the signal is more than 2 samples long, which are computed separately by using the FFT itself; if the number of even (and odd) samples are still more than 2 the samples are divided even further until the number of even (and odd) samples are 2. These samples are then computed by the DFT described in algorithm \ref{DFTalg}. After dividing the samples into even and odd samples the twiddle factor is multiplied appropiately (as described in section \ref{sec:FFT}), and eventually the Numpy-function \textit{concatenate} is used to join the arrays back together.

\begin{algorithm}
\caption{FFT algorithm}
\label{FFTalg}
\begin{algorithmic}[1]

\Procedure{Power of 2}{N}
	\State{Return $N$! $= 0$ and $(N \& (N - 1)) == 0$}
\EndProcedure

\Procedure{Compute FFT}{$x$}
	\State $N$ = length of $x$ 
	\If{Power of 2$(N) ==$ False }
		\State{Raise Valueerror(``N should be a power of 2.'')}
	\ElsIf{$N == 2$}
		\State{Return DFT($x$,$N$)} \Comment{$x$ is the signal}
	\Else
		\State{$X_{e} = FFT(x[::2])$} \Comment{The even 				samples}
		\State{$X_{o} = FFT(x[1::2])$} \Comment{The odd 				samples}
		\State{$factor = \exp(-j\cdot 2\pi \cdot 						$np.arange$(N) / N)$} \Comment{The twiddle 			factor}
		\State{Return np.concatenate$([X_{e} + factor[: 	N/ 2] \cdot X_{o}, X_{e} +  factor[N / 2:] \cdot X_{o}])$}
	\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Test of the FFT}
The FFT algorithm above is compared to both the DFT algorithm and the FFT function from the Numpy-package by measuring how long it takes for the functions to compute different amounts of random numbers. This is shown in table \ref{tab:FTcompare}. Furthermore, the algorithms are compared by their outputs.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Type / $N$ & DFT	   & FFT 	 & Numpy's FFT \\ \hline
$2^9$  	   & $1.03$    & $0.01$  & $\sim 0.00$ \\ \hline
$2^{10}$   & $4.43$    & $0.02$  & $\sim 0.00$ \\ \hline
$2^{11}$   & $17.36$   & $0.03$  & $\sim 0.00$ \\ \hline
$2^{12}$   & $69.42$   & $0.07$  & $\sim 0.00$ \\ \hline
$2^{13}$   & $290.89$  & $0.18$  & $\sim 0.00$ \\ \hline
$2^{14}$   & $1206.85$ & $0.27$  & $\sim 0.00$ \\ \hline
\end{tabular}
\caption{Times in seconds it take for the implementations of the DFT, the FFT and Numpy's FFT to compute $N$ random numbers. Processor used: Intel Core I5 generation 5 with CPU of 2.20 GHz.}
\label{tab:FTcompare}
\end{table}

Table \ref{tab:FTcompare} shows that the time the DFT takes to compute the $N$ numbers grows rapidly, whereas either of the FFTs take less than a second. The values for Numpy's FFT are rounded off to zero, and this function's ability to rapidly perform the computations is due to the fact that Numpy actually performs the computations in C, where the code is already compiled and runs directly on the CPU. On the other hand, Python is an interpreted language, which means that each line of the code has to be translated before it can be run. However, that obviously does not mean that the implementation of the FFT shown above is slow, and it is certainly much faster than the implementation of the DFT.
\\ \\
It is furthermore interesting to examine whether this result corresponds to the theoretical relation of the computational complexities. From section \ref{sec:FFT} the theoretical numbers of arithmetic operations required to compute the DFT and FFT are $N^2$ and $N\log_2(N)$, respectively. The theoretical relation $\frac{N^2}{N \cdot \log_2(N)}$ of these computational complexities equals $\frac{N}{\nu}$ for $N = 2^\nu, \nu \in \mathbb{N}$. This is shown in table \ref{tab:com_compare} together with the practical relation of the computation times of the DFT and FFT from table \ref{tab:FTcompare}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Relation / $N$	& Theoretical 	& Practical	& Difference		\\ \hline
$2^9$  	   		& $56.89$    	& $103.00$	& $46.11$	\\ \hline
$2^{10}$   		& $102.40$    	& $221.50$	& $119.10$	\\ \hline
$2^{11}$   		& $186.18$   	& $558.67$	& $392.48$	\\ \hline
$2^{12}$   		& $341.33$   	& $991.71$	& $650.38$	\\ \hline
$2^{13}$   		& $630.15$  	& $1616.06$	& $985.90$	\\ \hline
$2^{14}$   		& $1170.29$ 	& $4469.81$	& $3299.52$	\\ \hline
\end{tabular}
\caption{Comparison of the theoretical relation in computations and practical relation in computation times between the DFT and FFT.}
\label{tab:com_compare}
\end{table}

Therefore, for e.g. $N = 2^9$ the FFT is theoretically $56.89$ times faster than the DFT but in practice it is $103$ times faster. This is because the number of computations and the computer's processing time cannot be directly compared.
\\ \\
The implementation of the FFT and Numpy's FFT should also be compared for larger number of samples $N$ since e.g. $N = 2^{19}$ is 11.89 seconds with a sampling frequency of 44100 Hz, which is a reasonable length of an audio file from the recordings described in appendix \ref{recordings}. The DFT will obviously take too long and is therefore not included in the comparison in table \ref{tab:FT2compare}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Type / $N$ & FFT	   & Numpy's FFT \\ \hline
$2^{18}$   & $5.15$    & $0.03$ \\ \hline
$2^{19}$   & $9.79$    & $0.04$ \\ \hline
$2^{20}$   & $19.74$   & $0.14$ \\ \hline
$2^{21}$   & $40.31$   & $0.26$ \\ \hline
$2^{22}$   & $79.59$   & $0.56$ \\ \hline
$2^{23}$   & $159.39$  & $1.08$ \\ \hline
\end{tabular}
\caption{Times in seconds it take for the implementation of the FFT and Numpy's FFT to compute $N$ random numbers. Processor used: Intel Core I5 generation 5 with CPU of 2.20 GHz.}
\label{tab:FT2compare}
\end{table}

Obviously, the implementation of the FFT takes rather long for large numbers of samples, whereas Numpy's FFT only takes a little more than a second. It should be noted that the results above cannot be reproduced exactly since the numbers are random and the same amount of different numbers results in slightly different times of computations. Also, one might check the validity of the DFT and FFT algorithms by comparing the result of the algorithms and Numpy's FFT by using the Numpy function \textit{np.allclose}. An example is \textit{np.allclose(FFT(x),np.fft.fft(x))}, which returns a True-statement if the results of $FFT(x)$ described in algorithm \ref{FFTalg} and Numpy's FFT are similar to a certain degree$^[$\footnote{See also the documentation for the \textit{allclose}-function on \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html}{https://docs.scipy.org}.}$^]$. This was always the case in the comparison of the implementations of the DFT, the FFT and Numpy's FFT described above.
\\ \\
Therefore, the implementation of the DFT and FFT described in algorithm \ref{DFTalg} and \ref{FFTalg}, respectively, returns the exact same values as Numpy's FFT. The DFT is obviously extremely slow for even rather small numbers of samples, and the FFT is also rather slow compared to Numpy's FFT for large numbers of samples.
\\ \\
Because Numpy's FFT is so much faster than the implemented FFT and because Numpy's FFT is capable of zeropadding and thus computing a signal of any length, it is decided to use Numpy's FFT in the future processings of the signals in this report.